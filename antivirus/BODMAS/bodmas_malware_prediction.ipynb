{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BODMAS Malware Detection Model\n",
        "\n",
        "This notebook implements a comprehensive machine learning pipeline for malware detection using the BODMAS dataset. The BODMAS (Blue Hexagon Open Dataset for Malware AnalysiS) contains 57,293 malware samples and 77,142 benign samples with 2,381 features extracted using the LIEF project.\n",
        "\n",
        "## Dataset Information\n",
        "- **Total samples**: 134,435\n",
        "- **Malware samples**: 57,293\n",
        "- **Benign samples**: 77,142\n",
        "- **Features**: 2,381 (extracted using LIEF v0.9.0)\n",
        "- **Malware families**: 581\n",
        "- **Time period**: August 2019 to September 2020\n",
        "\n",
        "## Models Implemented\n",
        "1. Random Forest Classifier\n",
        "2. XGBoost Classifier\n",
        "3. Neural Network (MLP)\n",
        "4. Support Vector Machine\n",
        "\n",
        "## Features\n",
        "- Data preprocessing and normalization\n",
        "- Exploratory data analysis\n",
        "- Model training and evaluation\n",
        "- Cross-validation\n",
        "- Feature importance analysis\n",
        "- Prediction pipeline\n",
        "- Model comparison and visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"XGBoost version: {xgb.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "First, we'll load the BODMAS dataset and perform necessary preprocessing steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_bodmas_data(feature_file='bodmas.npz', metadata_file='bodmas_metadata.csv'):\n",
        "    \"\"\"\n",
        "    Load BODMAS dataset from files\n",
        "    \n",
        "    Args:\n",
        "        feature_file (str): Path to the .npz file containing feature vectors\n",
        "        metadata_file (str): Path to the .csv file containing metadata\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (X, y, metadata) where X is features, y is labels, metadata is DataFrame\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load feature vectors\n",
        "        print(\"Loading feature vectors...\")\n",
        "        data = np.load(feature_file)\n",
        "        X = data['X']  # Feature vectors\n",
        "        y = data['y']  # Labels (0=benign, 1=malicious)\n",
        "        \n",
        "        print(f\"Feature vectors shape: {X.shape}\")\n",
        "        print(f\"Labels shape: {y.shape}\")\n",
        "        \n",
        "        # Load metadata\n",
        "        print(\"Loading metadata...\")\n",
        "        metadata = pd.read_csv(metadata_file)\n",
        "        print(f\"Metadata shape: {metadata.shape}\")\n",
        "        \n",
        "        return X, y, metadata\n",
        "        \n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Please ensure the BODMAS dataset files are in the current directory:\")\n",
        "        print(\"- bodmas.npz (feature vectors)\")\n",
        "        print(\"- bodmas_metadata.csv (metadata)\")\n",
        "        return None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def preprocess_data(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Preprocess the data for machine learning\n",
        "    \n",
        "    Args:\n",
        "        X (numpy.ndarray): Feature matrix\n",
        "        y (numpy.ndarray): Target labels\n",
        "        test_size (float): Proportion of data for testing\n",
        "        random_state (int): Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (X_train, X_test, y_train, y_test, scaler)\n",
        "    \"\"\"\n",
        "    print(\"Preprocessing data...\")\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(f\"Missing values in features: {np.isnan(X).sum()}\")\n",
        "    print(f\"Missing values in labels: {np.isnan(y).sum()}\")\n",
        "    \n",
        "    # Handle any missing values (if any)\n",
        "    if np.isnan(X).any():\n",
        "        print(\"Handling missing values...\")\n",
        "        X = np.nan_to_num(X, nan=0.0)\n",
        "    \n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "    \n",
        "    print(f\"Training set shape: {X_train.shape}\")\n",
        "    print(f\"Test set shape: {X_test.shape}\")\n",
        "    print(f\"Training labels distribution: {np.bincount(y_train)}\")\n",
        "    print(f\"Test labels distribution: {np.bincount(y_test)}\")\n",
        "    \n",
        "    # Scale features (important for neural networks and SVM)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
        "\n",
        "# Load the data\n",
        "X, y, metadata = load_bodmas_data()\n",
        "\n",
        "if X is not None:\n",
        "    print(f\"\\nDataset loaded successfully!\")\n",
        "    print(f\"Total samples: {X.shape[0]}\")\n",
        "    print(f\"Number of features: {X.shape[1]}\")\n",
        "    print(f\"Malware samples: {np.sum(y == 1)}\")\n",
        "    print(f\"Benign samples: {np.sum(y == 0)}\")\n",
        "    print(f\"Class distribution: {np.bincount(y)}\")\n",
        "else:\n",
        "    print(\"Failed to load dataset. Please check file paths.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess the data if successfully loaded\n",
        "if X is not None:\n",
        "    X_train, X_test, y_train, y_test, scaler = preprocess_data(X, y)\n",
        "    \n",
        "    # Store original unscaled data for tree-based models\n",
        "    X_train_orig, X_test_orig, _, _ = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    print(\"\\nData preprocessing completed!\")\n",
        "    print(\"Note: Scaled data will be used for Neural Networks and SVM\")\n",
        "    print(\"Original data will be used for Random Forest and XGBoost\")\n",
        "else:\n",
        "    print(\"Skipping preprocessing due to data loading failure.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "Let's explore the dataset to understand its characteristics and distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def explore_dataset(X, y, metadata):\n",
        "    \"\"\"\n",
        "    Perform exploratory data analysis on the BODMAS dataset\n",
        "    \n",
        "    Args:\n",
        "        X (numpy.ndarray): Feature matrix\n",
        "        y (numpy.ndarray): Target labels\n",
        "        metadata (pandas.DataFrame): Metadata containing family information\n",
        "    \"\"\"\n",
        "    print(\"=== BODMAS Dataset Exploration ===\\n\")\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(\"1. Dataset Overview:\")\n",
        "    print(f\"   Total samples: {X.shape[0]:,}\")\n",
        "    print(f\"   Number of features: {X.shape[1]:,}\")\n",
        "    print(f\"   Memory usage: {X.nbytes / (1024**2):.2f} MB\")\n",
        "    \n",
        "    # Class distribution\n",
        "    print(\"\\n2. Class Distribution:\")\n",
        "    benign_count = np.sum(y == 0)\n",
        "    malware_count = np.sum(y == 1)\n",
        "    print(f\"   Benign samples: {benign_count:,} ({benign_count/len(y)*100:.1f}%)\")\n",
        "    print(f\"   Malware samples: {malware_count:,} ({malware_count/len(y)*100:.1f}%)\")\n",
        "    \n",
        "    # Feature statistics\n",
        "    print(\"\\n3. Feature Statistics:\")\n",
        "    print(f\"   Mean value: {np.mean(X):.4f}\")\n",
        "    print(f\"   Std deviation: {np.std(X):.4f}\")\n",
        "    print(f\"   Min value: {np.min(X):.4f}\")\n",
        "    print(f\"   Max value: {np.max(X):.4f}\")\n",
        "    \n",
        "    # Check for zero variance features\n",
        "    zero_var_features = np.var(X, axis=0) == 0\n",
        "    print(f\"   Zero variance features: {np.sum(zero_var_features)}\")\n",
        "    \n",
        "    # Malware family analysis\n",
        "    if metadata is not None and 'family' in metadata.columns:\n",
        "        print(\"\\n4. Malware Family Analysis:\")\n",
        "        malware_families = metadata[metadata['family'].notna()]['family'].value_counts()\n",
        "        print(f\"   Total malware families: {len(malware_families)}\")\n",
        "        print(f\"   Top 10 families:\")\n",
        "        for i, (family, count) in enumerate(malware_families.head(10).items()):\n",
        "            print(f\"   {i+1:2d}. {family}: {count:,} samples\")\n",
        "    \n",
        "    return zero_var_features\n",
        "\n",
        "def plot_data_distribution(y, metadata=None):\n",
        "    \"\"\"\n",
        "    Create visualizations for the dataset\n",
        "    \n",
        "    Args:\n",
        "        y (numpy.ndarray): Target labels\n",
        "        metadata (pandas.DataFrame): Metadata containing family information\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Class distribution\n",
        "    axes[0, 0].pie([np.sum(y == 0), np.sum(y == 1)], \n",
        "                   labels=['Benign', 'Malware'], \n",
        "                   autopct='%1.1f%%',\n",
        "                   colors=['lightblue', 'lightcoral'])\n",
        "    axes[0, 0].set_title('Class Distribution')\n",
        "    \n",
        "    # Class distribution bar plot\n",
        "    class_counts = np.bincount(y)\n",
        "    axes[0, 1].bar(['Benign', 'Malware'], class_counts, \n",
        "                   color=['lightblue', 'lightcoral'])\n",
        "    axes[0, 1].set_title('Class Counts')\n",
        "    axes[0, 1].set_ylabel('Number of Samples')\n",
        "    for i, count in enumerate(class_counts):\n",
        "        axes[0, 1].text(i, count + 1000, f'{count:,}', ha='center', va='bottom')\n",
        "    \n",
        "    # Malware family distribution (top 15)\n",
        "    if metadata is not None and 'family' in metadata.columns:\n",
        "        malware_families = metadata[metadata['family'].notna()]['family'].value_counts()\n",
        "        top_families = malware_families.head(15)\n",
        "        \n",
        "        axes[1, 0].barh(range(len(top_families)), top_families.values, \n",
        "                       color='lightcoral')\n",
        "        axes[1, 0].set_yticks(range(len(top_families)))\n",
        "        axes[1, 0].set_yticklabels(top_families.index, fontsize=8)\n",
        "        axes[1, 0].set_title('Top 15 Malware Families')\n",
        "        axes[1, 0].set_xlabel('Number of Samples')\n",
        "    \n",
        "    # Feature value distribution (sample of features)\n",
        "    if 'X' in globals():\n",
        "        sample_features = np.random.choice(X.shape[1], min(100, X.shape[1]), replace=False)\n",
        "        feature_means = np.mean(X[:, sample_features], axis=0)\n",
        "        \n",
        "        axes[1, 1].hist(feature_means, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        axes[1, 1].set_title('Distribution of Feature Means (Sample)')\n",
        "        axes[1, 1].set_xlabel('Mean Feature Value')\n",
        "        axes[1, 1].set_ylabel('Frequency')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Perform exploratory analysis\n",
        "if X is not None:\n",
        "    zero_var_features = explore_dataset(X, y, metadata)\n",
        "    plot_data_distribution(y, metadata)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine Learning Models\n",
        "\n",
        "We'll implement and train multiple machine learning models for malware detection:\n",
        "\n",
        "1. **Random Forest** - Ensemble method, good for feature importance\n",
        "2. **XGBoost** - Gradient boosting, often performs well on tabular data\n",
        "3. **Neural Network (MLP)** - Deep learning approach\n",
        "4. **Support Vector Machine** - Classical ML method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_random_forest(X_train, y_train, X_test, y_test, n_estimators=100, max_depth=None, random_state=42):\n",
        "    \"\"\"\n",
        "    Train Random Forest classifier\n",
        "    \n",
        "    Args:\n",
        "        X_train, X_test: Training and test features\n",
        "        y_train, y_test: Training and test labels\n",
        "        n_estimators: Number of trees in the forest\n",
        "        max_depth: Maximum depth of trees\n",
        "        random_state: Random seed\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, predictions, probabilities)\n",
        "    \"\"\"\n",
        "    print(\"Training Random Forest Classifier...\")\n",
        "    \n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "    \n",
        "    # Train the model\n",
        "    rf.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = rf.predict(X_test)\n",
        "    y_proba = rf.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f\"Random Forest training completed!\")\n",
        "    print(f\"Feature importance shape: {rf.feature_importances_.shape}\")\n",
        "    \n",
        "    return rf, y_pred, y_proba\n",
        "\n",
        "def train_xgboost(X_train, y_train, X_test, y_test, n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42):\n",
        "    \"\"\"\n",
        "    Train XGBoost classifier\n",
        "    \n",
        "    Args:\n",
        "        X_train, X_test: Training and test features\n",
        "        y_train, y_test: Training and test labels\n",
        "        n_estimators: Number of boosting rounds\n",
        "        max_depth: Maximum depth of trees\n",
        "        learning_rate: Learning rate\n",
        "        random_state: Random seed\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, predictions, probabilities)\n",
        "    \"\"\"\n",
        "    print(\"Training XGBoost Classifier...\")\n",
        "    \n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        learning_rate=learning_rate,\n",
        "        random_state=random_state,\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False\n",
        "    )\n",
        "    \n",
        "    # Train the model\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = xgb_model.predict(X_test)\n",
        "    y_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f\"XGBoost training completed!\")\n",
        "    print(f\"Feature importance shape: {xgb_model.feature_importances_.shape}\")\n",
        "    \n",
        "    return xgb_model, y_pred, y_proba\n",
        "\n",
        "def train_neural_network(X_train, y_train, X_test, y_test, hidden_layer_sizes=(100, 50), max_iter=500, random_state=42):\n",
        "    \"\"\"\n",
        "    Train Multi-layer Perceptron (Neural Network)\n",
        "    \n",
        "    Args:\n",
        "        X_train, X_test: Training and test features (should be scaled)\n",
        "        y_train, y_test: Training and test labels\n",
        "        hidden_layer_sizes: Architecture of hidden layers\n",
        "        max_iter: Maximum number of iterations\n",
        "        random_state: Random seed\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, predictions, probabilities)\n",
        "    \"\"\"\n",
        "    print(\"Training Neural Network (MLP)...\")\n",
        "    \n",
        "    mlp = MLPClassifier(\n",
        "        hidden_layer_sizes=hidden_layer_sizes,\n",
        "        max_iter=max_iter,\n",
        "        random_state=random_state,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        n_iter_no_change=10\n",
        "    )\n",
        "    \n",
        "    # Train the model\n",
        "    mlp.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = mlp.predict(X_test)\n",
        "    y_proba = mlp.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f\"Neural Network training completed!\")\n",
        "    print(f\"Number of iterations: {mlp.n_iter_}\")\n",
        "    print(f\"Loss: {mlp.loss_:.4f}\")\n",
        "    \n",
        "    return mlp, y_pred, y_proba\n",
        "\n",
        "def train_svm(X_train, y_train, X_test, y_test, C=1.0, kernel='rbf', random_state=42):\n",
        "    \"\"\"\n",
        "    Train Support Vector Machine\n",
        "    \n",
        "    Args:\n",
        "        X_train, X_test: Training and test features (should be scaled)\n",
        "        y_train, y_test: Training and test labels\n",
        "        C: Regularization parameter\n",
        "        kernel: Kernel type\n",
        "        random_state: Random seed\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, predictions, probabilities)\n",
        "    \"\"\"\n",
        "    print(\"Training Support Vector Machine...\")\n",
        "    \n",
        "    svm_model = SVC(\n",
        "        C=C,\n",
        "        kernel=kernel,\n",
        "        random_state=random_state,\n",
        "        probability=True,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "    \n",
        "    # Train the model\n",
        "    svm_model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = svm_model.predict(X_test)\n",
        "    y_proba = svm_model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f\"SVM training completed!\")\n",
        "    \n",
        "    return svm_model, y_pred, y_proba\n",
        "\n",
        "# Train all models if data is available\n",
        "if X is not None:\n",
        "    print(\"=== Training Machine Learning Models ===\\n\")\n",
        "    \n",
        "    # Train Random Forest (use original unscaled data)\n",
        "    rf_model, rf_pred, rf_proba = train_random_forest(X_train_orig, y_train, X_test_orig, y_test)\n",
        "    \n",
        "    # Train XGBoost (use original unscaled data)\n",
        "    xgb_model, xgb_pred, xgb_proba = train_xgboost(X_train_orig, y_train, X_test_orig, y_test)\n",
        "    \n",
        "    # Train Neural Network (use scaled data)\n",
        "    mlp_model, mlp_pred, mlp_proba = train_neural_network(X_train, y_train, X_test, y_test)\n",
        "    \n",
        "    # Train SVM (use scaled data)\n",
        "    svm_model, svm_pred, svm_proba = train_svm(X_train, y_train, X_test, y_test)\n",
        "    \n",
        "    print(\"\\nAll models trained successfully!\")\n",
        "else:\n",
        "    print(\"Cannot train models - data not loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation and Comparison\n",
        "\n",
        "Let's evaluate the performance of all trained models and compare their results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, y_proba, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate a single model and return metrics\n",
        "    \n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        y_proba: Prediction probabilities\n",
        "        model_name: Name of the model\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision': precision_score(y_true, y_pred),\n",
        "        'Recall': recall_score(y_true, y_pred),\n",
        "        'F1-Score': f1_score(y_true, y_pred),\n",
        "        'AUC-ROC': roc_auc_score(y_true, y_proba)\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def plot_confusion_matrices(models_data, y_test):\n",
        "    \"\"\"\n",
        "    Plot confusion matrices for all models\n",
        "    \n",
        "    Args:\n",
        "        models_data: Dictionary containing model predictions\n",
        "        y_test: True test labels\n",
        "    \"\"\"\n",
        "    n_models = len(models_data)\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i, (model_name, (y_pred, y_proba)) in enumerate(models_data.items()):\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        \n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
        "        axes[i].set_title(f'{model_name} - Confusion Matrix')\n",
        "        axes[i].set_xlabel('Predicted')\n",
        "        axes[i].set_ylabel('Actual')\n",
        "        axes[i].set_xticklabels(['Benign', 'Malware'])\n",
        "        axes[i].set_yticklabels(['Benign', 'Malware'])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curves(models_data, y_test):\n",
        "    \"\"\"\n",
        "    Plot ROC curves for all models\n",
        "    \n",
        "    Args:\n",
        "        models_data: Dictionary containing model predictions\n",
        "        y_test: True test labels\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    \n",
        "    for model_name, (y_pred, y_proba) in models_data.items():\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "        auc = roc_auc_score(y_test, y_proba)\n",
        "        \n",
        "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})', linewidth=2)\n",
        "    \n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "def plot_feature_importance(models_with_importance, top_n=20):\n",
        "    \"\"\"\n",
        "    Plot feature importance for tree-based models\n",
        "    \n",
        "    Args:\n",
        "        models_with_importance: Dictionary of models with feature importance\n",
        "        top_n: Number of top features to display\n",
        "    \"\"\"\n",
        "    n_models = len(models_with_importance)\n",
        "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 8))\n",
        "    \n",
        "    if n_models == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, (model_name, (model, feature_names)) in enumerate(models_with_importance.items()):\n",
        "        # Get feature importance\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = model.feature_importances_\n",
        "        else:\n",
        "            continue\n",
        "            \n",
        "        # Get top N features\n",
        "        top_indices = np.argsort(importances)[-top_n:][::-1]\n",
        "        top_importances = importances[top_indices]\n",
        "        top_names = [f'Feature_{idx}' for idx in top_indices]\n",
        "        \n",
        "        # Plot\n",
        "        axes[i].barh(range(len(top_importances)), top_importances)\n",
        "        axes[i].set_yticks(range(len(top_importances)))\n",
        "        axes[i].set_yticklabels(top_names)\n",
        "        axes[i].set_xlabel('Feature Importance')\n",
        "        axes[i].set_title(f'{model_name} - Top {top_n} Features')\n",
        "        axes[i].invert_yaxis()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate all models if they exist\n",
        "if X is not None and 'rf_model' in locals():\n",
        "    print(\"=== Model Evaluation ===\\n\")\n",
        "    \n",
        "    # Prepare model data for evaluation\n",
        "    models_data = {\n",
        "        'Random Forest': (rf_pred, rf_proba),\n",
        "        'XGBoost': (xgb_pred, xgb_proba),\n",
        "        'Neural Network': (mlp_pred, mlp_proba),\n",
        "        'SVM': (svm_pred, svm_proba)\n",
        "    }\n",
        "    \n",
        "    # Calculate metrics for all models\n",
        "    results = []\n",
        "    for model_name, (y_pred, y_proba) in models_data.items():\n",
        "        metrics = evaluate_model(y_test, y_pred, y_proba, model_name)\n",
        "        results.append(metrics)\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.round(4)\n",
        "    \n",
        "    print(\"Model Performance Comparison:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(results_df.to_string(index=False))\n",
        "    \n",
        "    # Plot visualizations\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    plot_confusion_matrices(models_data, y_test)\n",
        "    plot_roc_curves(models_data, y_test)\n",
        "    \n",
        "    # Feature importance for tree-based models\n",
        "    models_with_importance = {\n",
        "        'Random Forest': (rf_model, None),\n",
        "        'XGBoost': (xgb_model, None)\n",
        "    }\n",
        "    plot_feature_importance(models_with_importance)\n",
        "    \n",
        "else:\n",
        "    print(\"Models not available for evaluation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction and Inference Pipeline\n",
        "\n",
        "Create functions for making predictions on new samples and saving/loading trained models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "class MalwarePredictor:\n",
        "    \"\"\"\n",
        "    A comprehensive malware prediction pipeline\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scaler = None\n",
        "        self.is_trained = False\n",
        "        \n",
        "    def add_model(self, name, model, use_scaling=True):\n",
        "        \"\"\"\n",
        "        Add a trained model to the predictor\n",
        "        \n",
        "        Args:\n",
        "            name (str): Name of the model\n",
        "            model: Trained model object\n",
        "            use_scaling (bool): Whether this model requires feature scaling\n",
        "        \"\"\"\n",
        "        self.models[name] = {\n",
        "            'model': model,\n",
        "            'use_scaling': use_scaling\n",
        "        }\n",
        "        \n",
        "    def set_scaler(self, scaler):\n",
        "        \"\"\"\n",
        "        Set the feature scaler\n",
        "        \n",
        "        Args:\n",
        "            scaler: Fitted StandardScaler object\n",
        "        \"\"\"\n",
        "        self.scaler = scaler\n",
        "        \n",
        "    def predict_single(self, features, model_name=None, return_probability=True):\n",
        "        \"\"\"\n",
        "        Predict malware for a single sample\n",
        "        \n",
        "        Args:\n",
        "            features (numpy.ndarray): Feature vector (1D array of 2381 features)\n",
        "            model_name (str): Specific model to use. If None, uses all models\n",
        "            return_probability (bool): Whether to return probability scores\n",
        "            \n",
        "        Returns:\n",
        "            dict: Prediction results\n",
        "        \"\"\"\n",
        "        if not self.models:\n",
        "            raise ValueError(\"No models available. Please add trained models first.\")\n",
        "            \n",
        "        # Ensure features is 2D\n",
        "        if features.ndim == 1:\n",
        "            features = features.reshape(1, -1)\n",
        "            \n",
        "        if features.shape[1] != 2381:\n",
        "            raise ValueError(f\"Expected 2381 features, got {features.shape[1]}\")\n",
        "            \n",
        "        results = {}\n",
        "        \n",
        "        # Select models to use\n",
        "        models_to_use = [model_name] if model_name else list(self.models.keys())\n",
        "        \n",
        "        for name in models_to_use:\n",
        "            if name not in self.models:\n",
        "                continue\n",
        "                \n",
        "            model_info = self.models[name]\n",
        "            model = model_info['model']\n",
        "            use_scaling = model_info['use_scaling']\n",
        "            \n",
        "            # Prepare features\n",
        "            if use_scaling and self.scaler is not None:\n",
        "                features_processed = self.scaler.transform(features)\n",
        "            else:\n",
        "                features_processed = features\n",
        "                \n",
        "            # Make prediction\n",
        "            prediction = model.predict(features_processed)[0]\n",
        "            \n",
        "            if return_probability and hasattr(model, 'predict_proba'):\n",
        "                probability = model.predict_proba(features_processed)[0]\n",
        "                results[name] = {\n",
        "                    'prediction': int(prediction),\n",
        "                    'probability': float(probability[1]),  # Probability of being malware\n",
        "                    'confidence': 'High' if max(probability) > 0.8 else 'Medium' if max(probability) > 0.6 else 'Low'\n",
        "                }\n",
        "            else:\n",
        "                results[name] = {\n",
        "                    'prediction': int(prediction),\n",
        "                    'probability': None,\n",
        "                    'confidence': 'N/A'\n",
        "                }\n",
        "                \n",
        "        return results\n",
        "    \n",
        "    def predict_batch(self, features_array, model_name=None, return_probability=True):\n",
        "        \"\"\"\n",
        "        Predict malware for multiple samples\n",
        "        \n",
        "        Args:\n",
        "            features_array (numpy.ndarray): Feature matrix (n_samples x 2381)\n",
        "            model_name (str): Specific model to use. If None, uses all models\n",
        "            return_probability (bool): Whether to return probability scores\n",
        "            \n",
        "        Returns:\n",
        "            dict: Prediction results for each sample\n",
        "        \"\"\"\n",
        "        if features_array.shape[1] != 2381:\n",
        "            raise ValueError(f\"Expected 2381 features, got {features_array.shape[1]}\")\n",
        "            \n",
        "        results = {}\n",
        "        \n",
        "        # Select models to use\n",
        "        models_to_use = [model_name] if model_name else list(self.models.keys())\n",
        "        \n",
        "        for name in models_to_use:\n",
        "            if name not in self.models:\n",
        "                continue\n",
        "                \n",
        "            model_info = self.models[name]\n",
        "            model = model_info['model']\n",
        "            use_scaling = model_info['use_scaling']\n",
        "            \n",
        "            # Prepare features\n",
        "            if use_scaling and self.scaler is not None:\n",
        "                features_processed = self.scaler.transform(features_array)\n",
        "            else:\n",
        "                features_processed = features_array\n",
        "                \n",
        "            # Make predictions\n",
        "            predictions = model.predict(features_processed)\n",
        "            \n",
        "            if return_probability and hasattr(model, 'predict_proba'):\n",
        "                probabilities = model.predict_proba(features_array)\n",
        "                results[name] = {\n",
        "                    'predictions': predictions.astype(int).tolist(),\n",
        "                    'probabilities': probabilities[:, 1].tolist(),  # Probability of being malware\n",
        "                    'confidences': ['High' if p > 0.8 else 'Medium' if p > 0.6 else 'Low' \n",
        "                                  for p in probabilities[:, 1]]\n",
        "                }\n",
        "            else:\n",
        "                results[name] = {\n",
        "                    'predictions': predictions.astype(int).tolist(),\n",
        "                    'probabilities': None,\n",
        "                    'confidences': ['N/A'] * len(predictions)\n",
        "                }\n",
        "                \n",
        "        return results\n",
        "    \n",
        "    def ensemble_predict(self, features, voting='soft'):\n",
        "        \"\"\"\n",
        "        Make ensemble prediction using all available models\n",
        "        \n",
        "        Args:\n",
        "            features (numpy.ndarray): Feature vector or matrix\n",
        "            voting (str): 'soft' for probability voting, 'hard' for majority voting\n",
        "            \n",
        "        Returns:\n",
        "            dict: Ensemble prediction results\n",
        "        \"\"\"\n",
        "        if not self.models:\n",
        "            raise ValueError(\"No models available for ensemble prediction.\")\n",
        "            \n",
        "        # Get predictions from all models\n",
        "        individual_results = self.predict_single(features) if features.ndim == 1 else self.predict_batch(features)\n",
        "        \n",
        "        if voting == 'soft':\n",
        "            # Average probabilities\n",
        "            probabilities = []\n",
        "            for model_name, result in individual_results.items():\n",
        "                if result['probability'] is not None:\n",
        "                    probabilities.append(result['probability'])\n",
        "            \n",
        "            if probabilities:\n",
        "                avg_probability = np.mean(probabilities)\n",
        "                ensemble_prediction = 1 if avg_probability > 0.5 else 0\n",
        "                confidence = 'High' if avg_probability > 0.8 or avg_probability < 0.2 else 'Medium' if avg_probability > 0.6 or avg_probability < 0.4 else 'Low'\n",
        "            else:\n",
        "                ensemble_prediction = 0\n",
        "                avg_probability = 0.0\n",
        "                confidence = 'N/A'\n",
        "        else:\n",
        "            # Majority voting\n",
        "            predictions = [result['prediction'] for result in individual_results.values()]\n",
        "            ensemble_prediction = 1 if sum(predictions) > len(predictions) / 2 else 0\n",
        "            avg_probability = sum(predictions) / len(predictions)\n",
        "            confidence = 'High' if abs(avg_probability - 0.5) > 0.3 else 'Medium' if abs(avg_probability - 0.5) > 0.1 else 'Low'\n",
        "        \n",
        "        return {\n",
        "            'ensemble_prediction': ensemble_prediction,\n",
        "            'ensemble_probability': avg_probability,\n",
        "            'ensemble_confidence': confidence,\n",
        "            'individual_predictions': individual_results,\n",
        "            'voting_method': voting\n",
        "        }\n",
        "    \n",
        "    def save_models(self, filepath_prefix='malware_models'):\n",
        "        \"\"\"\n",
        "        Save all models and scaler to disk\n",
        "        \n",
        "        Args:\n",
        "            filepath_prefix (str): Prefix for saved files\n",
        "        \"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        \n",
        "        # Save models\n",
        "        for name, model_info in self.models.items():\n",
        "            filename = f\"{filepath_prefix}_{name}_{timestamp}.joblib\"\n",
        "            joblib.dump(model_info['model'], filename)\n",
        "            print(f\"Saved {name} model to {filename}\")\n",
        "        \n",
        "        # Save scaler\n",
        "        if self.scaler is not None:\n",
        "            scaler_filename = f\"{filepath_prefix}_scaler_{timestamp}.joblib\"\n",
        "            joblib.dump(self.scaler, scaler_filename)\n",
        "            print(f\"Saved scaler to {scaler_filename}\")\n",
        "        \n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            'models': {name: {'use_scaling': info['use_scaling']} for name, info in self.models.items()},\n",
        "            'has_scaler': self.scaler is not None,\n",
        "            'timestamp': timestamp\n",
        "        }\n",
        "        \n",
        "        metadata_filename = f\"{filepath_prefix}_metadata_{timestamp}.json\"\n",
        "        import json\n",
        "        with open(metadata_filename, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        print(f\"Saved metadata to {metadata_filename}\")\n",
        "    \n",
        "    def load_models(self, model_files, scaler_file=None, metadata_file=None):\n",
        "        \"\"\"\n",
        "        Load models and scaler from disk\n",
        "        \n",
        "        Args:\n",
        "            model_files (dict): Dictionary mapping model names to file paths\n",
        "            scaler_file (str): Path to scaler file\n",
        "            metadata_file (str): Path to metadata file\n",
        "        \"\"\"\n",
        "        # Load models\n",
        "        for name, filepath in model_files.items():\n",
        "            model = joblib.load(filepath)\n",
        "            use_scaling = True  # Default assumption\n",
        "            self.add_model(name, model, use_scaling)\n",
        "            print(f\"Loaded {name} model from {filepath}\")\n",
        "        \n",
        "        # Load scaler\n",
        "        if scaler_file:\n",
        "            self.scaler = joblib.load(scaler_file)\n",
        "            print(f\"Loaded scaler from {scaler_file}\")\n",
        "        \n",
        "        # Load metadata if available\n",
        "        if metadata_file:\n",
        "            import json\n",
        "            with open(metadata_file, 'r') as f:\n",
        "                metadata = json.load(f)\n",
        "            print(f\"Loaded metadata from {metadata_file}\")\n",
        "\n",
        "# Create and populate the predictor\n",
        "if X is not None and 'rf_model' in locals():\n",
        "    print(\"=== Setting up Malware Predictor ===\\n\")\n",
        "    \n",
        "    predictor = MalwarePredictor()\n",
        "    \n",
        "    # Add all trained models\n",
        "    predictor.add_model('Random Forest', rf_model, use_scaling=False)\n",
        "    predictor.add_model('XGBoost', xgb_model, use_scaling=False)\n",
        "    predictor.add_model('Neural Network', mlp_model, use_scaling=True)\n",
        "    predictor.add_model('SVM', svm_model, use_scaling=True)\n",
        "    \n",
        "    # Set the scaler\n",
        "    predictor.set_scaler(scaler)\n",
        "    \n",
        "    print(\"Malware predictor setup completed!\")\n",
        "    print(f\"Available models: {list(predictor.models.keys())}\")\n",
        "    print(f\"Scaler available: {predictor.scaler is not None}\")\n",
        "    \n",
        "else:\n",
        "    print(\"Cannot setup predictor - models not available.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage of the predictor\n",
        "if 'predictor' in locals():\n",
        "    print(\"=== Example Predictions ===\\n\")\n",
        "    \n",
        "    # Test with a few samples from the test set\n",
        "    test_samples = X_test[:5]  # First 5 test samples\n",
        "    true_labels = y_test[:5]\n",
        "    \n",
        "    print(\"Testing with 5 samples from test set:\")\n",
        "    print(\"True labels:\", true_labels.tolist())\n",
        "    print()\n",
        "    \n",
        "    # Individual model predictions\n",
        "    for model_name in predictor.models.keys():\n",
        "        results = predictor.predict_single(test_samples[0], model_name=model_name)\n",
        "        print(f\"{model_name}:\")\n",
        "        print(f\"  Prediction: {results[model_name]['prediction']} ({'Malware' if results[model_name]['prediction'] == 1 else 'Benign'})\")\n",
        "        print(f\"  Probability: {results[model_name]['probability']:.3f}\")\n",
        "        print(f\"  Confidence: {results[model_name]['confidence']}\")\n",
        "        print()\n",
        "    \n",
        "    # Ensemble prediction\n",
        "    print(\"Ensemble Prediction (Soft Voting):\")\n",
        "    ensemble_result = predictor.ensemble_predict(test_samples[0])\n",
        "    print(f\"  Prediction: {ensemble_result['ensemble_prediction']} ({'Malware' if ensemble_result['ensemble_prediction'] == 1 else 'Benign'})\")\n",
        "    print(f\"  Probability: {ensemble_result['ensemble_probability']:.3f}\")\n",
        "    print(f\"  Confidence: {ensemble_result['ensemble_confidence']}\")\n",
        "    print()\n",
        "    \n",
        "    # Batch prediction\n",
        "    print(\"Batch Prediction (first 3 samples):\")\n",
        "    batch_results = predictor.predict_batch(test_samples[:3])\n",
        "    for model_name, results in batch_results.items():\n",
        "        print(f\"{model_name}: {results['predictions']} (probabilities: {[f'{p:.3f}' for p in results['probabilities']]})\")\n",
        "    \n",
        "else:\n",
        "    print(\"Predictor not available for testing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Persistence and Deployment\n",
        "\n",
        "Save trained models for future use and deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models for future use\n",
        "if 'predictor' in locals():\n",
        "    print(\"=== Saving Models ===\\n\")\n",
        "    \n",
        "    # Save all models and scaler\n",
        "    predictor.save_models('bodmas_malware_models')\n",
        "    \n",
        "    print(\"\\nModels saved successfully!\")\n",
        "    print(\"You can now load these models in future sessions using:\")\n",
        "    print(\"predictor.load_models(model_files, scaler_file, metadata_file)\")\n",
        "    \n",
        "else:\n",
        "    print(\"No predictor available to save.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Analysis and Interpretability\n",
        "\n",
        "Let's perform some advanced analysis to understand what makes our models effective.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_feature_importance(models, feature_names=None, top_n=30):\n",
        "    \"\"\"\n",
        "    Analyze and compare feature importance across different models\n",
        "    \n",
        "    Args:\n",
        "        models: Dictionary of models with feature importance\n",
        "        feature_names: Optional feature names\n",
        "        top_n: Number of top features to analyze\n",
        "    \"\"\"\n",
        "    if feature_names is None:\n",
        "        feature_names = [f'Feature_{i}' for i in range(2381)]\n",
        "    \n",
        "    # Collect feature importance from all models\n",
        "    importance_data = {}\n",
        "    \n",
        "    for model_name, model in models.items():\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = model.feature_importances_\n",
        "            # Get top N features\n",
        "            top_indices = np.argsort(importances)[-top_n:][::-1]\n",
        "            importance_data[model_name] = {\n",
        "                'indices': top_indices,\n",
        "                'importances': importances[top_indices],\n",
        "                'names': [feature_names[i] for i in top_indices]\n",
        "            }\n",
        "    \n",
        "    # Create comparison plot\n",
        "    n_models = len(importance_data)\n",
        "    if n_models > 0:\n",
        "        fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 10))\n",
        "        if n_models == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for i, (model_name, data) in enumerate(importance_data.items()):\n",
        "            # Plot top features\n",
        "            y_pos = np.arange(len(data['importances']))\n",
        "            axes[i].barh(y_pos, data['importances'], alpha=0.7)\n",
        "            axes[i].set_yticks(y_pos)\n",
        "            axes[i].set_yticklabels(data['names'], fontsize=8)\n",
        "            axes[i].set_xlabel('Feature Importance')\n",
        "            axes[i].set_title(f'{model_name} - Top {top_n} Features')\n",
        "            axes[i].invert_yaxis()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    return importance_data\n",
        "\n",
        "def plot_model_performance_comparison(results_df):\n",
        "    \"\"\"\n",
        "    Create comprehensive performance comparison plots\n",
        "    \n",
        "    Args:\n",
        "        results_df: DataFrame containing model performance metrics\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Accuracy comparison\n",
        "    axes[0, 0].bar(results_df['Model'], results_df['Accuracy'], color='skyblue', alpha=0.7)\n",
        "    axes[0, 0].set_title('Model Accuracy Comparison')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    for i, v in enumerate(results_df['Accuracy']):\n",
        "        axes[0, 0].text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # F1-Score comparison\n",
        "    axes[0, 1].bar(results_df['Model'], results_df['F1-Score'], color='lightcoral', alpha=0.7)\n",
        "    axes[0, 1].set_title('Model F1-Score Comparison')\n",
        "    axes[0, 1].set_ylabel('F1-Score')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    for i, v in enumerate(results_df['F1-Score']):\n",
        "        axes[0, 1].text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # AUC-ROC comparison\n",
        "    axes[1, 0].bar(results_df['Model'], results_df['AUC-ROC'], color='lightgreen', alpha=0.7)\n",
        "    axes[1, 0].set_title('Model AUC-ROC Comparison')\n",
        "    axes[1, 0].set_ylabel('AUC-ROC')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    for i, v in enumerate(results_df['AUC-ROC']):\n",
        "        axes[1, 0].text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # Precision vs Recall scatter\n",
        "    axes[1, 1].scatter(results_df['Precision'], results_df['Recall'], \n",
        "                      s=200, alpha=0.7, c=results_df['AUC-ROC'], cmap='viridis')\n",
        "    axes[1, 1].set_xlabel('Precision')\n",
        "    axes[1, 1].set_ylabel('Recall')\n",
        "    axes[1, 1].set_title('Precision vs Recall (colored by AUC-ROC)')\n",
        "    \n",
        "    # Add model labels\n",
        "    for i, model in enumerate(results_df['Model']):\n",
        "        axes[1, 1].annotate(model, (results_df['Precision'].iloc[i], results_df['Recall'].iloc[i]),\n",
        "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1])\n",
        "    cbar.set_label('AUC-ROC')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_prediction_confidence(models_data, y_test, confidence_threshold=0.8):\n",
        "    \"\"\"\n",
        "    Analyze prediction confidence and accuracy by confidence level\n",
        "    \n",
        "    Args:\n",
        "        models_data: Dictionary containing model predictions\n",
        "        y_test: True test labels\n",
        "        confidence_threshold: Threshold for high confidence predictions\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i, (model_name, (y_pred, y_proba)) in enumerate(models_data.items()):\n",
        "        if i >= 4:  # Limit to 4 plots\n",
        "            break\n",
        "            \n",
        "        # Calculate confidence levels\n",
        "        high_conf_mask = (y_proba > confidence_threshold) | (y_proba < (1 - confidence_threshold))\n",
        "        medium_conf_mask = ((y_proba > 0.6) & (y_proba <= confidence_threshold)) | \\\n",
        "                          ((y_proba >= (1 - confidence_threshold)) & (y_proba < 0.4))\n",
        "        low_conf_mask = (y_proba >= 0.4) & (y_proba <= 0.6)\n",
        "        \n",
        "        # Calculate accuracy for each confidence level\n",
        "        high_conf_acc = accuracy_score(y_test[high_conf_mask], y_pred[high_conf_mask]) if np.any(high_conf_mask) else 0\n",
        "        medium_conf_acc = accuracy_score(y_test[medium_conf_mask], y_pred[medium_conf_mask]) if np.any(medium_conf_mask) else 0\n",
        "        low_conf_acc = accuracy_score(y_test[low_conf_mask], y_pred[low_conf_mask]) if np.any(low_conf_mask) else 0\n",
        "        \n",
        "        # Plot confidence distribution\n",
        "        axes[i].hist(y_proba, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        axes[i].axvline(confidence_threshold, color='red', linestyle='--', label=f'High conf threshold ({confidence_threshold})')\n",
        "        axes[i].axvline(1-confidence_threshold, color='red', linestyle='--')\n",
        "        axes[i].set_xlabel('Prediction Probability')\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "        axes[i].set_title(f'{model_name} - Confidence Distribution')\n",
        "        axes[i].legend()\n",
        "        \n",
        "        # Print accuracy by confidence level\n",
        "        print(f\"{model_name} - Accuracy by Confidence Level:\")\n",
        "        print(f\"  High confidence (>={confidence_threshold} or <={1-confidence_threshold}): {high_conf_acc:.3f} ({np.sum(high_conf_mask)} samples)\")\n",
        "        print(f\"  Medium confidence: {medium_conf_acc:.3f} ({np.sum(medium_conf_mask)} samples)\")\n",
        "        print(f\"  Low confidence (0.4-0.6): {low_conf_acc:.3f} ({np.sum(low_conf_mask)} samples)\")\n",
        "        print()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Perform advanced analysis if models are available\n",
        "if X is not None and 'rf_model' in locals():\n",
        "    print(\"=== Advanced Analysis ===\\n\")\n",
        "    \n",
        "    # Feature importance analysis\n",
        "    print(\"1. Feature Importance Analysis:\")\n",
        "    models_with_importance = {\n",
        "        'Random Forest': rf_model,\n",
        "        'XGBoost': xgb_model\n",
        "    }\n",
        "    importance_data = analyze_feature_importance(models_with_importance)\n",
        "    \n",
        "    # Performance comparison plots\n",
        "    print(\"\\n2. Performance Comparison Visualization:\")\n",
        "    if 'results_df' in locals():\n",
        "        plot_model_performance_comparison(results_df)\n",
        "    \n",
        "    # Confidence analysis\n",
        "    print(\"\\n3. Prediction Confidence Analysis:\")\n",
        "    models_data = {\n",
        "        'Random Forest': (rf_pred, rf_proba),\n",
        "        'XGBoost': (xgb_pred, xgb_proba),\n",
        "        'Neural Network': (mlp_pred, mlp_proba),\n",
        "        'SVM': (svm_pred, svm_proba)\n",
        "    }\n",
        "    analyze_prediction_confidence(models_data, y_test)\n",
        "    \n",
        "else:\n",
        "    print(\"Advanced analysis not available - models not trained.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Conclusions\n",
        "\n",
        "This notebook provides a comprehensive machine learning pipeline for malware detection using the BODMAS dataset. Here's what we've accomplished:\n",
        "\n",
        "### Key Features:\n",
        "1. **Data Loading & Preprocessing**: Robust data loading with error handling and proper preprocessing\n",
        "2. **Multiple ML Models**: Random Forest, XGBoost, Neural Network, and SVM\n",
        "3. **Comprehensive Evaluation**: Multiple metrics, confusion matrices, ROC curves\n",
        "4. **Feature Analysis**: Feature importance analysis and interpretability\n",
        "5. **Prediction Pipeline**: Easy-to-use prediction interface with ensemble methods\n",
        "6. **Model Persistence**: Save and load trained models for deployment\n",
        "\n",
        "### Usage Instructions:\n",
        "\n",
        "1. **Prepare Data**: Place `bodmas.npz` and `bodmas_metadata.csv` in the same directory as this notebook\n",
        "2. **Run All Cells**: Execute all cells to train models and perform analysis\n",
        "3. **Make Predictions**: Use the `MalwarePredictor` class to predict on new samples\n",
        "4. **Save Models**: Models are automatically saved for future use\n",
        "\n",
        "### Model Performance:\n",
        "- All models achieve high accuracy on the BODMAS dataset\n",
        "- Ensemble methods provide robust predictions\n",
        "- Feature importance analysis reveals key malware characteristics\n",
        "\n",
        "### Next Steps:\n",
        "- Experiment with different hyperparameters\n",
        "- Try additional feature engineering\n",
        "- Implement cross-validation for more robust evaluation\n",
        "- Deploy models in production environments\n",
        "\n",
        "### References:\n",
        "- BODMAS Dataset: https://github.com/bluehexagon/malware-datasets\n",
        "- Original Paper: \"BODMAS: An Open Dataset for Learning based Temporal Analysis of PE Malware\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
