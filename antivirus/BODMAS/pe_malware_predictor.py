#!/usr/bin/env python3
"""
PE Malware Predictor

A Python script for predicting malware in PE files using trained BODMAS models.
This script extracts features from PE files using LIEF and applies trained ML models.

Usage:
    python pe_malware_predictor.py <pe_file_path>
    python pe_malware_predictor.py <pe_file_path> --model <model_name>
    python pe_malware_predictor.py <pe_file_path> --ensemble
"""

import argparse
import sys
import os
import numpy as np
import joblib
import json
from pathlib import Path

try:
    import lief
except ImportError:
    print("Error: LIEF library not found. Please install it with: pip install lief==0.9.0")
    sys.exit(1)

try:
    from sklearn.preprocessing import StandardScaler
    from sklearn.calibration import CalibratedClassifierCV
except ImportError:
    print("Error: scikit-learn not found. Please install it with: pip install scikit-learn")
    sys.exit(1)


class PEFeatureExtractor:
    """
    Extract features from PE files using LIEF library.
    This implementation follows the BODMAS dataset feature extraction process.
    """
    
    def __init__(self):
        self.feature_names = []
        self._initialize_feature_names()
    
    def _initialize_feature_names(self):
        """Initialize the expected feature names for BODMAS dataset."""
        # This is a simplified version - the actual BODMAS extraction is more complex
        # For production use, you'd need the exact feature extraction code from the dataset
        self.feature_names = [f"feature_{i}" for i in range(2381)]
    
    def extract_features(self, pe_file_path):
        """
        Extract features from a PE file.
        
        Args:
            pe_file_path (str): Path to the PE file
            
        Returns:
            numpy.ndarray: Feature vector of length 2381
        """
        try:
            # Parse PE file with LIEF
            binary = lief.parse(pe_file_path)
            if binary is None:
                raise ValueError(f"Failed to parse PE file: {pe_file_path}")
            
            # Initialize feature vector
            features = np.zeros(2381, dtype=np.float32)
            
            # Extract basic PE header features
            features[0] = binary.header.machine.value if binary.header.machine else 0
            features[1] = binary.header.characteristics
            features[2] = binary.header.time_date_stamps
            features[3] = binary.optional_header.addressof_entrypoint
            features[4] = binary.optional_header.imagebase
            features[5] = binary.optional_header.section_alignment
            features[6] = binary.optional_header.file_alignment
            features[7] = binary.optional_header.major_operating_system_version
            features[8] = binary.optional_header.minor_operating_system_version
            features[9] = binary.optional_header.major_image_version
            features[10] = binary.optional_header.minor_image_version
            features[11] = binary.optional_header.major_subsystem_version
            features[12] = binary.optional_header.minor_subsystem_version
            features[13] = binary.optional_header.win32_version_value
            features[14] = binary.optional_header.sizeof_image
            features[15] = binary.optional_header.sizeof_headers
            features[16] = binary.optional_header.checksum
            features[17] = binary.optional_header.subsystem.value if binary.optional_header.subsystem else 0
            features[18] = binary.optional_header.dll_characteristics
            features[19] = binary.optional_header.sizeof_stack_reserve
            features[20] = binary.optional_header.sizeof_stack_commit
            features[21] = binary.optional_header.sizeof_heap_reserve
            features[22] = binary.optional_header.sizeof_heap_commit
            features[23] = binary.optional_header.loader_flags
            features[24] = binary.optional_header.numberof_rva_and_sizes
            
            # Section features
            section_idx = 25
            for i, section in enumerate(binary.sections[:10]):  # Limit to first 10 sections
                if section_idx + 4 < len(features):
                    features[section_idx] = len(section.name) if section.name else 0
                    features[section_idx + 1] = section.virtual_size
                    features[section_idx + 2] = section.sizeof_raw_data
                    features[section_idx + 3] = section.characteristics
                    section_idx += 4
            
            # Import features
            import_idx = 65
            if binary.imports:
                features[import_idx] = len(binary.imports)
                import_idx += 1
                for i, imp in enumerate(binary.imports[:20]):  # Limit to first 20 imports
                    if import_idx + 2 < len(features):
                        features[import_idx] = len(imp.name) if imp.name else 0
                        features[import_idx + 1] = len(imp.entries)
                        import_idx += 2
            
            # Export features
            export_idx = 105
            if binary.has_exports:
                features[export_idx] = len(binary.get_export().entries)
                export_idx += 1
            
            # Resource features
            resource_idx = 106
            if binary.has_resources:
                features[resource_idx] = len(binary.resources)
                resource_idx += 1
            
            # Debug features
            debug_idx = 107
            if binary.has_debug:
                features[debug_idx] = len(binary.debug)
                debug_idx += 1
            
            # TLS features
            tls_idx = 108
            if binary.has_tls:
                features[tls_idx] = 1
                tls_idx += 1
            
            # Load configuration features
            lc_idx = 109
            if binary.has_configuration:
                features[lc_idx] = 1
                lc_idx += 1
            
            # Rich header features
            rich_idx = 110
            if binary.has_rich_header:
                features[rich_idx] = len(binary.rich_header.entries)
                rich_idx += 1
            
            # Digital signature features
            sig_idx = 111
            if binary.has_signatures:
                features[sig_idx] = len(binary.signatures)
                sig_idx += 1
            
            # Relocation features
            reloc_idx = 112
            if binary.has_relocations:
                features[reloc_idx] = len(binary.relocations)
                reloc_idx += 1
            
            # Fill remaining features with zeros (simplified approach)
            # In practice, you'd extract many more features to reach 2381
            
            return features
            
        except Exception as e:
            print(f"Error extracting features from {pe_file_path}: {e}")
            return np.zeros(2381, dtype=np.float32)


class PEMalwarePredictor:
    """
    Main class for PE malware prediction using trained BODMAS models.
    """
    
    def __init__(self):
        self.models = {}
        self.scaler = None
        self.feature_extractor = PEFeatureExtractor()
        self.model_metadata = {}
    
    def load_models(self, model_files, scaler_file=None, metadata_file=None):
        """
        Load trained models from disk.
        
        Args:
            model_files (dict): Dictionary mapping model names to file paths
            scaler_file (str): Path to scaler file
            metadata_file (str): Path to metadata file
        """
        # Load models
        for name, filepath in model_files.items():
            if os.path.exists(filepath):
                model = joblib.load(filepath)
                use_scaling = True  # Default assumption
                self.models[name] = {
                    'model': model,
                    'use_scaling': use_scaling
                }
                print(f"Loaded {name} model from {filepath}")
            else:
                print(f"Warning: Model file not found: {filepath}")
        
        # Load scaler
        if scaler_file and os.path.exists(scaler_file):
            self.scaler = joblib.load(scaler_file)
            print(f"Loaded scaler from {scaler_file}")
        
        # Load metadata if available
        if metadata_file and os.path.exists(metadata_file):
            with open(metadata_file, 'r') as f:
                self.model_metadata = json.load(f)
            print(f"Loaded metadata from {metadata_file}")
            
            # Update scaling requirements from metadata
            if 'models' in self.model_metadata:
                for name, info in self.model_metadata['models'].items():
                    if name in self.models:
                        self.models[name]['use_scaling'] = info.get('use_scaling', True)
    
    def predict_pe_file(self, file_path, model_name=None):
        """
        Predict malware for a single PE file.
        
        Args:
            file_path (str): Path to the PE file
            model_name (str): Specific model to use (optional)
            
        Returns:
            dict: Prediction results
        """
        if not self.models:
            raise ValueError("No models loaded. Please load models first.")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"PE file not found: {file_path}")
        
        # Extract features
        features = self.feature_extractor.extract_features(file_path)
        features = features.reshape(1, -1)
        
        # Select models to use
        models_to_use = [model_name] if model_name else list(self.models.keys())
        
        results = {}
        for name in models_to_use:
            if name not in self.models:
                continue
            
            model_info = self.models[name]
            model = model_info['model']
            use_scaling = model_info['use_scaling']
            
            # Prepare features
            if use_scaling and self.scaler is not None:
                features_processed = self.scaler.transform(features)
            else:
                features_processed = features
            
            # Make prediction
            prediction = model.predict(features_processed)[0]
            
            if hasattr(model, 'predict_proba'):
                probability = model.predict_proba(features_processed)[0]
                results[name] = {
                    'prediction': int(prediction),
                    'probability': float(probability[1]),  # Probability of being malware
                    'confidence': 'High' if max(probability) > 0.8 else 'Medium' if max(probability) > 0.6 else 'Low'
                }
            else:
                results[name] = {
                    'prediction': int(prediction),
                    'probability': None,
                    'confidence': 'N/A'
                }
        
        return results
    
    def predict_pe_files(self, file_paths, model_name=None):
        """
        Predict malware for multiple PE files.
        
        Args:
            file_paths (list): List of PE file paths
            model_name (str): Specific model to use (optional)
            
        Returns:
            dict: Prediction results for each file
        """
        results = {}
        for file_path in file_paths:
            try:
                results[file_path] = self.predict_pe_file(file_path, model_name)
            except Exception as e:
                results[file_path] = {'error': str(e)}
        
        return results
    
    def ensemble_predict_pe_file(self, file_path, voting='soft'):
        """
        Make ensemble prediction using all available models.
        
        Args:
            file_path (str): Path to the PE file
            voting (str): 'soft' for probability voting, 'hard' for majority voting
            
        Returns:
            dict: Ensemble prediction results
        """
        if not self.models:
            raise ValueError("No models loaded. Please load models first.")
        
        # Get predictions from all models
        individual_results = self.predict_pe_file(file_path)
        
        if voting == 'soft':
            # Average probabilities
            probabilities = []
            for model_name, result in individual_results.items():
                if result.get('probability') is not None:
                    probabilities.append(result['probability'])
            
            if probabilities:
                avg_probability = np.mean(probabilities)
                ensemble_prediction = 1 if avg_probability > 0.5 else 0
                confidence = 'High' if avg_probability > 0.8 or avg_probability < 0.2 else 'Medium' if avg_probability > 0.6 or avg_probability < 0.4 else 'Low'
            else:
                ensemble_prediction = 0
                avg_probability = 0.0
                confidence = 'N/A'
        else:
            # Majority voting
            predictions = [result['prediction'] for result in individual_results.values()]
            ensemble_prediction = 1 if sum(predictions) > len(predictions) / 2 else 0
            avg_probability = sum(predictions) / len(predictions)
            confidence = 'High' if abs(avg_probability - 0.5) > 0.3 else 'Medium' if abs(avg_probability - 0.5) > 0.1 else 'Low'
        
        return {
            'ensemble_prediction': ensemble_prediction,
            'ensemble_probability': avg_probability,
            'ensemble_confidence': confidence,
            'individual_predictions': individual_results,
            'voting_method': voting
        }


def main():
    """Command line interface for PE malware prediction."""
    parser = argparse.ArgumentParser(description='Predict malware in PE files using trained BODMAS models')
    parser.add_argument('pe_file', help='Path to the PE file to analyze')
    parser.add_argument('--model', choices=['Random Forest', 'XGBoost', 'Neural Network'], 
                       help='Specific model to use (default: all models)')
    parser.add_argument('--ensemble', action='store_true', 
                       help='Use ensemble prediction with all models')
    parser.add_argument('--model-dir', default='.', 
                       help='Directory containing model files (default: current directory)')
    
    args = parser.parse_args()
    
    # Initialize predictor
    predictor = PEMalwarePredictor()
    
    # Find model files
    model_dir = Path(args.model_dir)
    model_files = {}
    scaler_file = None
    metadata_file = None
    
    # Look for model files
    for pattern in ['*Random Forest*.joblib', '*XGBoost*.joblib', '*Neural Network*.joblib']:
        files = list(model_dir.glob(pattern))
        if files:
            model_name = 'Random Forest' if 'Random Forest' in pattern else 'XGBoost' if 'XGBoost' in pattern else 'Neural Network'
            model_files[model_name] = str(files[0])
    
    # Look for scaler and metadata files
    scaler_files = list(model_dir.glob('*scaler*.joblib'))
    if scaler_files:
        scaler_file = str(scaler_files[0])
    
    metadata_files = list(model_dir.glob('*metadata*.json'))
    if metadata_files:
        metadata_file = str(metadata_files[0])
    
    if not model_files:
        print("Error: No model files found. Please ensure model files are in the specified directory.")
        sys.exit(1)
    
    # Load models
    predictor.load_models(model_files, scaler_file, metadata_file)
    
    if not predictor.models:
        print("Error: No models could be loaded.")
        sys.exit(1)
    
    # Make prediction
    try:
        if args.ensemble:
            result = predictor.ensemble_predict_pe_file(args.pe_file)
            print(f"\n=== Ensemble Prediction ===")
            print(f"File: {args.pe_file}")
            print(f"Prediction: {'Malware' if result['ensemble_prediction'] == 1 else 'Benign'}")
            print(f"Probability: {result['ensemble_probability']:.3f}")
            print(f"Confidence: {result['ensemble_confidence']}")
            print(f"Voting Method: {result['voting_method']}")
            print(f"\nIndividual Model Results:")
            for model_name, model_result in result['individual_predictions'].items():
                print(f"  {model_name}: {'Malware' if model_result['prediction'] == 1 else 'Benign'} "
                      f"(prob: {model_result['probability']:.3f}, conf: {model_result['confidence']})")
        else:
            results = predictor.predict_pe_file(args.pe_file, args.model)
            print(f"\n=== Prediction Results ===")
            print(f"File: {args.pe_file}")
            for model_name, result in results.items():
                print(f"\n{model_name}:")
                print(f"  Prediction: {'Malware' if result['prediction'] == 1 else 'Benign'}")
                print(f"  Probability: {result['probability']:.3f}")
                print(f"  Confidence: {result['confidence']}")
    
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
